这里是golang拾遗系列，用来记录学习过程中一些生涩、遗漏的概念。

## 2.epoll

本节讨论 Golang 底层 io 模型中使用到的epoll机制。

走读Golang源码版本为1.19，全文目录树如下:

![image-20230227103528496](imgs\image-20230227103528496.png)

### 2.1IO 多路复用

**何为多路复用?**

拆解多路复用一词：

- 多路：存在多个待服务的对象
- 复用：只由一个执行单元提供服务

串联上述要点，多路复用指的是，由一个执行单元，同时对多个对象提供服务，形成一对多的服务关系。

例如：餐厅的一个服务员固定负责几个餐桌的用餐需求，辗转提供服务，本质上就是一种多路复用。

![image-20230227104227670](imgs\image-20230227104227670.png)

回到计算机领域， linux 操作系统对 IO 多路复用的概念有着更加明确的定义：

- 多路：存在多个需要处理 io event 的 fd（linux 中，一切皆文件，所有事务均可抽象为一个文件句柄 file descriptor，简称 fd）
- 复用：复用一个 loop thread 同时为多个 fd 提供处理服务（线程 thread 是内核视角下的最小调度单位；多路复用通常为循环模型 loop model，因此称为 loop thread）

解释完概念，再对 IO 多路复用附加一些约定俗称的要求：

IO 多路复用中，loop thread 是提供服务的乙方；待处理 io event 的 fd 们是甲方. 本着顾客是上帝的原则，乙方有义务为甲方提供更优质的服务，这里的服务质量就体现在一句话：”随叫随到，别让老板等久了”.

在餐厅顾客没有需求的时候，服务生趁着闲工夫摸个鱼打个盹也尚无不可. 当时一旦顾客招呼时，服务生需要第一时间赶到对需求作出响应.

此外，由于服务生和顾客之间的服务关系是一对多，所以还要考虑到有多名顾客同时招呼时，服务生如何作兼容处理，让每名顾客都不至于产生被冷落的感觉. 这是一门学问，也同样是计算机领域 IO 多路复用场景下需要解决的问题.

### 2.2多路复用的简单实现

#### 2.2.1 阻塞IO

下面通过一段伪代码，尝试让 IO 多路复用这个概念看起来更加具体一些：

```go
// 多个待服务的 fd 
fds = [fd1,fd2,fd3,...]
// 遍历 fd 列表，末尾和首部相连，形成循环
i = 0
for {
    // 获取本轮待处理的 fd
    fd = fds[i]        
    // 从 fd 中读数据
    data = read(fd)  
    // 处理数据 
    handle(data)             
    // 推进遍历
    i++
    if i == len(fds){
        i = 0
    }
}
```

上述搭了个架子，核心分为几步：

- 定义了待处理的 fds 列表（多路）
- 循环遍历 fds 列表，每轮负责读一个 fd（复用）

这个乞丐版的 IO 多路复用模型，看起来似乎有那么点意思了. 然而其本质上是一种阻塞 IO 模型（Blocking IO，简称 BIO）. 事实上，上述实现存在一个致命的问题，那就是句柄 fd 默认的 io 操作是阻塞型的，因此倘若在读 fd1 的时候，io event 没到达，那么 loop thread 就会陷入阻塞，后续 fd2、fd3 哪怕有 io event 到达，也无法得到执行.

上述问题翻译成更形象的场景，大概就是：

- A桌顾客对服务生说，你先搁这候着，我看会儿菜单，一会点菜
- 服务生于是站定A桌，打定主意在A桌点完菜之后再离开
- 在此期间，服务生辖区内的B桌、C桌招呼有事，服务生也充耳不闻，只等A桌事情完结才肯挪动步子

这样的服务显然不够到位，倘若人人如此，餐厅必然面临倒闭.

#### 2.2.2 非阻塞IO

基于 BIO 存在的问题，我们进行一轮改进。核心是将 read 操作由同步阻塞操作改为带有尝试性的非阻塞操作.。

在读一个 fd 的时候，倘若 io event 已就绪就正常读取，否则就即时返回并抛出一个特定类型的错误，让 loop thread 能够正常执行下去，为其他 fd 提供服务。

```go
// 多个待服务的 fd 
fds = [fd1,fd2,fd3,...]
// 遍历 fd 列表，末尾和首部相连，形成循环
i = 0
for {
    // 获取本轮待处理的 fd
    fd = fds[i]        
    // 尝试从 fd 中读数据，失败时不阻塞，而是抛出错误
    data,err = tryRead(fd)  
    // 读取数据成功，处理数据
    if err == nil{
        handle(data) 
    } 
    // 小睡一秒后再推进流程
    sleep(1 second)
    // 推进遍历
    i++
    if i == len(fds){
        i = 0
    }
}
```

上述伪代码核心步骤如下：

- 定义了待处理的 fds 列表
- 遍历 fds 列表，每轮尝试从一个 fd 中读数据
- 倘若 io event 已就绪，则正常处理结果
- 倘若 io event 未就绪，只抛出错误，同样不阻塞流程
- 小睡一会儿，然后继续推进流程

这里确实解决阻塞 IO 中的问题，其本质上是一种非阻塞 IO 模型（Nonblocking IO，简称 NIO）。但这里仍然存在问题，就是每轮处理之间的休眠时间。倘若在休眠期间，fd 中有 io event 到达，就无法被正常处理，这同样是一种不好的体验。

这一问题翻译成餐厅的场景，指的就是服务生每次主动问询或者为一名客人提供服务后，就要大喘气休息几分钟，期间对客人不管不顾，这样的服务态度客人同样不会买账。

那大家可能会问了，倘若把此处的休眠操作去除了如何？

答案是同样有问题。 倘若不限制轮询的执行频率，那么不轮 fd 中是否有 io event，程序都会一直高强度运行，这会导致 CPU 空转，造成很大程度的资源浪费。

用餐厅的场景来聊，指的是餐厅招了个视听都不好的服务生，他感应不到客人的召唤，需要时时刻刻奔走在各个餐桌之间主动去询问客人们是否需要服务。这种情况下，哪怕客人们性子好不嫌烦，服务生自己也被这种高强度的反复横跳动作给累坏了。

那大家可能又问了。餐厅就不能招个正常的服务生吗，让他在听到客人的招呼时就去提供服务，否则就在一边老实歇着。

没错，这就是正解，设计程序的码农们也是这么想的。然而实际情况很悲催，在用户态视角下的程序正是哪一个耳目昏聩的服务生，对于 io event 的到达并没有能力做到准确地把握。

于是，这就需要引入操作系统内核的帮助，通过内核对外暴露的几个接口，来进行 IO 多路复用的优雅实现，做到真正意义上的“随叫随到”。

### 2.3 IO多路复用的优雅实现

linux 内核提供了三种经典的多路复用技术：

![image-20230227105845362](imgs\image-20230227105845362.png)

从上图中可以看到，各个技术之间通过单向箭头连接，因此是一个持续演化改进的过程，select 最通用，但是相对粗糙；而 epoll 则最精致，在性能上也有着最优越的表现。

poll 在 select 的基础之上做了改进，但治标不治本，优化得不够彻底。我们核心还是来对比看看 select 和 epoll 之间的共性和差异：

#### 2.3.1 select

- 一次可以处理多个 fd，体现多路。但 fd 数量有限，最多 1024 个
- loop thread 通过 select 将一组 fd 提交到内核做监听
- 当 fd 中无 io event 就绪时，loop thread 会陷入阻塞
- 每当这组 fd 中有 io event 到达时，内核会唤醒 loop thread
- loop thread 无法精准感知到哪些 fd 就绪，需要遍历一轮 fd 列表，时间复杂度 O(N)
- 托付给内核的 fd 列表只具有一轮交互的时效. 新的轮次中，loop thread 需要重新将监听的 fd 列表再传递给内核一次

#### 2.3.2 epoll

- 每次处理的 fd 数量无上限
- loop thread 通过 epoll_create 操作创建一个 epoll 池子
- loop thread 通过 epoll_ctl 每次将一个待监听的 fd 添加到 epoll 池中
- 每当 fd 列表中有 fd 就绪事件到达时，会唤醒 loop thread。 同时内核会将处于就绪态的 fd 直接告知 loop thread，无需额外遍历

综上所述，select 和 epoll 等多路复用操作利用了内核的能力，能在待监听 fd 中有 io event 到达时，将 loop thread 唤醒，避免无意义的主动轮询操作.

其中，epoll 相比于 select 的核心性能优势在于：

- loop thread 被唤醒时，能明确知道哪些 fd 需要处理，减少了一次额外遍历的操作，时间复杂度由 O(N) 优化到 O(1)
- epoll 通过将创建池子和添加 fd两个操作解耦，实现了池中 fd 数据的复用，减少了用户态与内核态间的数据拷贝成本

### 2.4 EventPoll核心指令

epoll 又称 EventPoll，使用很简单，包含三个指令：

- epoll_create
- epoll_ctl
- epoll_wait

#### 2.4.1 epoll_create

在内核开辟空间，创建一个 epoll 池子用于批量存储管理 fd，后续可以通过 epoll_ctl 往池子中增删改 fd。

```go
func epollcreate1(flags int32) int32
```

#### 2.4.2 epoll_ctl

在某个 epoll 池子中进行一个 fd 的增删改操作。

正是由于 epoll 中将 epoll_ctl 与 epoll_create 操作进行了解耦，才实现了对 epoll_create 时传递的 fd 数据的复用，减少了用户态和内核态之间对 fd 数据的重复传递。

此外，在 epoll_ctl 实现时，也需要通过 epollevent 设置好回调事件，当 fd 有指定事件到达时，会被添加到就绪队列中，最终将 loop thread 唤醒。

```go
func epollctl(epfd, op, fd int32, ev *epollevent) int32

type epollevent struct {
    events uint32
    data   [8]byte // unaligned uintptr
}
```

#### 2.4.3 epoll_wait

从对应 epoll 池子中获取就绪的 epollevent，从中可以关联到对应的 fd 和 loop thread 信息.

```go
func epollwait(epfd int32, ev *epollevent, nev, timeout int32) int32
```

### 2.5 核心数据结构

#### 2.5.1 epoll 池红黑树

一个 epoll 池子中管理的 fd 数量理论上上不封顶。同时后续可能存在对 fd 的增删改操作，因此需要使用合适的数据结构加以管理，从而降低后续操作的时间复杂度。

linux 内核中，实现 epoll 池的数据结构采用的是红黑树（Red-Black Tree，一种自平衡二叉查找树，这里不作展开，感兴趣自行了解）实现，保证了所有增、删、改操作的平均时间复杂度维持在 O(logN) 的对数级水平。

![image-20230227155050776](imgs\image-20230227155050776.png)

#### 2.5.2 就绪事件队列

针对于 fd 的就绪 io event，由于通常数量有限，且每个事件都需要逐一处理，没有优先级之分，因此采用简单的双向链表实现即可。

![image-20230227155142901](imgs\image-20230227155142901.png)

### 2.6 事件回调机制

epoll 高效的核心建立在精准的事件回调机制之上.

首先，通过内核感知到 io event 事件的动态，令 loop thread 在合适的时机阻塞，避免浪费 CPU；在合适的时机执行，及时处理 io event.

其次，在 io event 就绪时，会精准地将真正就绪的 fd 传递到 loop thread 手中，减少了一次无意义的遍历查询动作.

事件回调的注册是在调用 epoll_ctl 添加 fd 时，此时会提前设置好对这个 fd 关心的事件类型，当对应的 io event 真的发生时，内核会将该 fd 和对应的 loop thread 封装到 epollevent 中，添加到就绪队列 ready list 当中.

之后当用户调用 epoll_wait 时，能够准确地获取到这部分就绪的 epollevent，进而能够将对应的 loop thread 唤醒.

### 2.7 Golang 网络 IO 源码走读

#### 2.7.1 启动TCP服务器

首先给出一个启动 tcp 服务的代码框架，伪代码如下：

```go
// 启动一个 tcp 服务端代码示例
func main(){
   // 创建一个 tcp 端口监听器
   l,_ := net.Listen("tcp",":8080")
   // 主动轮询模型
   for{
       // 等待 tcp 连接到达
       conn,_ := l.Accept()     
       // 开启一个 _goroutine 负责一笔客户端请求的处理
       go serve(conn)
   }
}

// 处理一笔 tcp 连接
func serve(conn net.Conn){
    defer conn.Close()
    var buf []byte
    // 读取连接中的数据
    _,_ = conn.Read(buf)    
    // ...
}
```

方法核心步骤都展示于 main 函数中了：

- 创建了一个 tcp 端口监听器 listener
- 通过 for 循环建立主动轮询模型
- 每轮尝试从 listener 中获取到达的 tcp 连接
- 倘若成功取到连接，则 1:1启动一个 goroutine 异步处理连接的请求
- 倘若无连接到达，则阻塞主流程

其中，有两个方法是核心入口：一个是创建 Listener 的 net.Listen；另一个是从 Listener 获取连接的 Listener.Accept 方法。

#### 2.7.2 创建TCP端口监听器

![image-20230227165431093](imgs\image-20230227165431093.png)

https://mp.weixin.qq.com/s/xt0Elppc_OaDFnTI_tW3hg

